pramen {
  # Define jobs as factory classes:
  # jobs = [ za.co.absa.pramen.framework.EdwToParquetSyncJob ]

  # Generate INFO file for Enceladus
  plugins {
    info.file {
      generate = false

      source.application = "Unspecified"
      country = "Africa"
      history.type = "Snapshot"

      timestamp.format = "dd-MM-yyyy HH:mm:ss Z"
      date.format = "yyyy-MM-dd"
    }
  }
}

# Define parameters for SyncJob
# pramen.jobs = [ za.co.absa.pramen.builtin.JDBCToParquetSyncJob ]
jdbc.sync {
  job.name = "JDBC to Parquet Sync"

  # Define a reader
  reader {
    jdbc {
      #   driver = "oracle.jdbc.OracleDriver"
      #
      #   # The primary URL that will be tried first (could be empty if all URLs have equal precedence)
      #   connection.string = ""
      #   or
      #   connection.primary.url = ""
      #
      #   # URL pool (If primary is specified, the pool is used as fallback, otherwise a random URL is selected):
      #   connection.url.1 = ""
      #   connection.url.2 = ""
      #   connection.url.3 = ""
      #
      #   database = ""
      #   user = ""
      #   password = ""
    }
    has.information.date.column = true

    information.date.column = "INFORMATION_DATE"
    # Data type of the information date column.
    # Currenty supported: date, string, number
    information.date.type = "date"
    information.date.app.format = "yyyy-MM-dd"
    information.date.sql.format = "YYYY-MM-DD"

    # Limit number of records read
    # limit.records = 100

    # Convert timestamp columns to dates (workaround for Oracle driver's schema compatibility)
    # save.timestamps.as.dates = false
    # Convert decimals with no scale to integers and longs, fix 'NUMBER' SQL to Spark mapping.
    # correct.decimals.in.schema = false
    # Fix the input precision interpretation (fixes errors like "Decimal precision 14 exceeds max precision 13")
    # correct.decimals.fix.precision = false

    # The number of JDBC connection retries
    # connection.retries = 3

    # Extra options to the Spark reader
    # option. =
    # For example,
    option.fetchsize = 50000
    # option.batchsize = 50000

    # SQL dialect-specific options
    # sql.
  }

  # Supported schedule types:

  # schedule.type = daily

  # schedule.type = weekly
  # 1 - Monday, ..., 7 - Sunday
  # schedule.days.of.week = [ 7 ]

  # schedule.type = monthly
  # schedule.days.of.month = [ 1 ]

  # The number of days to subtract from the job date before populating INFO_DATE column
  info.date.delay.days = 0

  # Add a processing timestamp column to the output table
  # processing.timestamp.col =

  writer.parquet {
    # snapshot.date.col =
    # snapshot.date.format = "yyyy-MM-dd"
    # snapshot.date.delay.days
  }


  # Define tables to sync (old way)
  # table.1 = [ table_name_1, /output/path/table1, <records_per_partition> ]
  # table.2 = [ table_name_2, /output/path/table2, 1000000 ]
  # ...

  # Define tables to sync (new way)
  # tables {
  #  table.1 {
  #    db.table = table_name_1
  #    pramen.table = table_name_1
  #    columns = [ column1, "cast(column2, date) as column2", column3, ... ]
  #    output.path = /output/path/table2
  #    records.per.partition = 1000000
  #    # You can specify an SQL query. This supports native SQL queries, including stored procedures execution.
  #    # It might be much less efficient. Use this as the last resort.
  #    sql = SELECT * FROM A
  #  }
  #}

}

#pramen.transformation {
#  factory.class = com.example.MyTransformationJob
#  job.name = "Job Name"
#
#  default.information.date.column = "INFORMATION_DATE"
#  default.information.date.format = "yyyy-MM-dd"
#
#  input.tables {
#    table.1 {
#      # Table name stored in bookkeeping
#      name = employees
#      path = "/path/to/the/table2"
#
#      # If true, the job can proceed even this table is not up to date.
#      # If all tables are optional, at least one table must be up to date for a job to proceed.
#      optional = false
#
#      # Specify the input period for the table
#      # @activeDate is the date the job running.
#      input.date.begin = "minusMonths(@activeDate, 1)"
#      input.date.end = "lastDayOfMonth(@activeDate)"
#
#      # Use these options if they are different from the default ones
#      # information.date.column = "INFORMATION_DATE"
#      # information.date.format = "yyyy-MM-dd"
#    }
#
#    table.2 {
#      name = check_weekly
#      path = "/path/to/the/table2"
#      has.info.date = false
#      optional = true
#    }
#  }
#
#  output.table {
#    name = "output.table.name"
#
#    path = "/path/to/output/table"
#
#    # Specify how the output information date should be calculated.
#    output.info.date = "@activeDate - 1"
#
#    records.per.partition = 1000000
#
#    # Use these options if they are different from the default ones
#    #information.column.name = "INFORMATION_DATE"
#    #information.column.format = "yyyy-MM-dd"
#  }
#}

#pramen.aggregation {
#  factory.class = za.co.absa.pramen.mocks.DummyAggregationJob
#  job.name = "Job Name"
#
#  schedule.type = "monthly"
#  schedule.days.of.month = [ 2 ]
#
#  input.tables = [ table1, table2 ]
#
#  output.table = "output.table.name"
#  output.info.date = "@activeDate - 2"
#}

# Define parameters for Parque to Kafka sync
# pramen.jobs = [ za.co.absa.pramen.builtin.ParquetToKafkaSyncJob ]
parquet.kafka.sync {
  job.name = "JDBC to Parquet Sync"

  # Define a writer
  writer.kafka {
    # brokers = ""

    # Any othe options supported by kafka
    # option.kafka.sasl.jaas.config =
    # option.kafka.sasl.mechanism = "GSSAPI"
    # option.kafka.security.protocol = "SASL_PLAINTEXT"
    # option.kafka.ssl.key.password =
    # option.kafka.ssl.keystore.location =
    # option.kafka.ssl.keystore.password =
    # option.kafka.ssl.truststore.location =
    # option.kafka.ssl.truststore.password =

    # schema.registry.url = ""

    # Define value naming strategy and its properties.
    # Can be one of "topic.name", "record.name", "topic.record.name"
    schema.registry.value.naming.strategy = "topic.name"
    # schema.registry.value.schema.record.name =
    # schema.registry.value.schema.record.namespace =
    # schema.registry.value.schema.id =

    # Define key naming strategy and its properties (key.column.names must be defined).
    # Can be one of "topic.name", "record.name", "topic.record.name"
    # schema.registry.key.naming.strategy = "topic.name"
    # schema.registry.key.schema.record.name =
    # schema.registry.key.schema.record.namespace =
    # schema.registry.key.schema.id =

    # Columns to use for the key
    # key.column.names =

    # Any othe options supported by the schema registry
    # schema.registry.extra =

    # Limit number of output records (for testing/staging purposes)
    # limit.records =
  }

  # Supported schedule types:

  # schedule.type = daily

  # schedule.type = weekly
  # 1 - Monday, ..., 7 - Sunday
  # schedule.days.of.week = [ 7 ]

  # schedule.type = monthly
  # schedule.days.of.month = [ 1 ]

  # Define tables to sync
  # table.1 = [ table_name_1, /input/path/table1, topic_name_1 ]
  # table.2 = [ table_name_2, /input/path/table2, topic_name_2 ]
  # ...

  # Or
  # table.3 = [ table_name_2, /input/path/table2, topic_name_2, output_table ]
}

cmd.line {
  job.name = "Bash script job"

  # Supported schedule types:

  # schedule.type = daily

  # schedule.type = weekly
  # 1 - Monday, ..., 7 - Sunday
  # schedule.days.of.week = [ 7 ]

  # schedule.type = monthly
  # schedule.days.of.month = [ 1 ]

  # The number of days to subtract from the end of information period to calculate output information date
  info.date.delay.days = 0

  # Define input tables (job dependencies)
  # The tables should be recent (not older than current date miunus 'expected.delay.days')
  # Example: input.tables = [ companies, employees, contacts ]
  input.table.names = []

  # Define output table namee (for bookkeeping)
  # output.table.name =

  # Script to run (local file system)
  # The script should return zero exit code on success and non-zero exit code on failure.
  # You can use the folloving substitution variables:
  # ${@infoDateBegin}      - Information date start of the period
  # ${@infoDateEnd}        - Information date end of the period
  # ${@infoDate}           - Output information date
  # ${monthOf(@infoDate)}  - Month (in yyyy-MM format) of the output information date

  # command =

  # The number of last log lines to include if the job has failed
  log.lines.to.include = 200

  # Matches record count line like this: OutputRecordCount=1000
  # record.count.regex = "^OutputRecordCount\\s*=\\s*(.*)$"

  # Matches the successfull job run that produces 0 records
  # zero.records.success.regex = "No\\srecords\\ssaved"

  # Matches a job failure due to no data for the information date
  # no.data.failure.regex = "Input\\sis\\snot\\sready"

  # Do not log and output lines that match any of the RexEx expressions in the list
  # output.filter.regex.1 =
  # output.filter.regex.2 =
}

# Define parameters for SyncJob
sync {
  # Define a reader
  # reader.jdbc {
  #   driver = "oracle.jdbc.OracleDriver"
  #   connection.string = ""
  #   user = ""
  #   password = ""
  # }

  # Supported schedule types:

  # schedule = daily

  # schedule = weekly
  # 1 - Monday, ..., 7 - Sunday
  # schedule.days.of.week = [ 7 ]

  # schedule = monthly
  # schedule.days.of.month = [ 1 ]

  # Define tables to sync
  # table.1 = [ table_name_1, /output/path/table1, <records_per_partition> ]
  # table.2 = [ table_name_2, /output/path/table1, 1000000 ]
  # ...
}
