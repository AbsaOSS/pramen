# Copyright 2022 ABSA Group Limited
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# This variable is expected to be set up by the test suite
#base.path = "/tmp"

pramen {
  pipeline.name = "Integration test with a multi-table transformers"

  bookkeeping.enabled = false
  stop.spark.session = false
}

pramen.metastore {
  tables = [
    {
      name = "table1"
      description = "Table 1"
      format = "parquet"
      path = ${base.path}/table1
    }
  ]
}

pramen.sources.1 = [
  {
    name = "spark_source"
    factory.class = "za.co.absa.pramen.core.source.SparkSource"

    has.information.date.column = false
  }
]

pramen.sinks.1 = [
  {
    name = "spark_sink"
    factory.class = "za.co.absa.pramen.core.sink.SparkSink"

    format = "parquet"
    mode = "overwrite"
  }
]

pramen.operations = [
  {
    name = "Loading data from Spark Catalog"
    type = "ingestion"
    schedule.type = "weekly"
    schedule.days.of.week = [ 1, 7 ] # On Sundays and Mondays

    source = "spark_source"

    info.date.expr = "@runDate"

    tables = [
      {
        input.table = "my_table1"
        output.metastore.table = table1
      }
    ]
  },
  {
    name = "Generate some data from the transformer"
    type = "transformation"

    class = "za.co.absa.pramen.core.mocks.transformer.GeneratingTransformer"
    schedule.type = "weekly"
    schedule.days.of.week = [ 6, 7 ] # On Saturdays and Sundays

    info.date.expr = "@runDate"

    output.table = "table1"

    dependencies = [ ]
  },
  {
    name = "Sinking to parquet"
    type = "sink"
    sink = "spark_sink"
    schedule.type = "daily"

    output.table = "table2"

    tables = [
      {
        input.metastore.table = table1
        output.path = ${base.path}"/sink"
      }
    ]
  }
]
