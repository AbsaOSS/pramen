# Copyright 2022 ABSA Group Limited
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

pramen {
  # This is set up automaticallly on build.
  application.version = "${project.version}"
  build.timestamp = "${build.timestamp}"

  ingestion.name = "Unspecified"
  environment.name = "DEV"

  bookkeeping.enabled = "true"

  ## This specifies the location to keep locks and bookkeeping. One of the following must be specified.

  # Use PostgresSQL for bookkeeping
  # bookkeeping.jdbc {
  #   driver = "org.postgresql.Driver"
  #   url = "jdbc:postgresql://host:5433/syncpramen"
  #   user = "username"
  #   password = "password"
  # }

  # Use MongoDB for bookkeeping
  # bookkeeping.mongodb.connection.string = "mongodb://127.0.0.1"
  # bookkeeping.mongodb.database = "syncpramen"

  # Use Hadoop (HDFS, S3, etc) for bookkeeping
  # bookkeeping.location = ""

  # Bookkeeping storage format: "delta" or "text"
  bookkeeping.hadoop.format = "text"

  # Default information column date field used for the metastore. Sourced tables will be partitioned by this field.
  information.date.column = "pramen_info_date"
  information.date.format = "yyyy-MM-dd"
  information.date.start = "2020-01-01"

  # Custom partitioning folder pattern (for Parquet output)
  # Default:
  # parquet.custom.partition.pattern = "{column}={year}-{month}-{day}"
  # For Enceladus:
  # parquet.custom.partition.pattern = "{year}/{month}/{day}/v1"

  # If non-zero, specifies how many tasks can be ran in parallel
  parallel.tasks = 1

  # If enabled, the job will wait for the output table to become available before running a job
  # If the number of seconds <=0 the waiting will be infinite
  wait.for.output.table.enabled = false
  wait.for.output.table.seconds = 600

  # How many days to check back for late data
  track.days = 4

  # Do not expect data to arrive specified number of days from now
  expected.delay.days = 0

  # Period for input tables is shifted (posifive - forward, negative - backward) from the target information date
  # For example,
  #   If current information date is 2020-09-01
  #     when 'input.period.shift.days = 0'  the input tables will be fetched 2020-08-02 to 2020-09-01 (inclusuve)
  #     when 'input.period.shift.days = -1' the input tables will be fetched 2020-08-01 to 2020-08-31 (inclusuve)
  input.period.shift.days = 0

  # Specifies an expression for ingestion job reading range
  #input.period.from.expr = "@infoDate"
  #input.period.to.expr = "@infoDate"

  # 1 - Mon, 7 - Sun
  schedule.weekly.days.of.week = [ 7 ]

  temporary.directory = ""

  warn.throughput.rps = 2000
  good.throughput.rps = 40000

  dry.run = false

  use.lock = true

  warn.if.no.data = true
  non.zero.exit.code.if.no.data = false

  # Send an email even if there are no changes and no late or not ready data
  email.if.no.changes = true

  check.only.late.data = false
  check.only.new.data = false

  ignore.source.deletions = false
  ignore.last.updated.date = false

  # If this is set the workflow will be re-run for the specified information date.
  #rerun.info.date =

  # If this is set the current date will overridden by the specified value.
  #current.date =

  # Always overwrite last data chunk.
  always.overwrite.last.chunk = false

  # If true, SyncWcatcer will track updates and can rewrite chunks to previusly written information dates
  track.updates = true

  # Pass arbitrary Spark Configuration when initializing Spark Session
  # For example, alternative way of writing legacy parquet will be
  # spark.conf.option.spark.sql.parquet.writeLegacyFormat = true
  #
  #spark.conf.option. =

  # Other option(s) might be
  # spark.conf.option.spark.sql.parquet.binaryAsString = true

  # Define data availability requirements for input tables.
  #input.data.availability {
  #  check.1 {
  #    # Specify either all tables should be available for the period, and/or one of the list if sufficient
  #    tables.all = [ table1, table2, table3 ]
  #    tables.one.of = [ table1, table2, table3 ]
  #
  #    # You may specify date 'from' (mandaory) and date 'to' (optional).
  #    date.from = "beginOfMonth(minusMonths(@infoDate, 1))"
  #    date.to = "endOfMonth(minusMonths(@infoDate, 1))"
  #  }
  #}

  # Define output information date expression (Obsolete configuration, use 'default.*.output.info.date.expr' instead)
  #output.info.date.expr = "@infoDate"

  # Default infroamation date expression for daily jobs
  default.daily.output.info.date.expr = "@runDate"

  # Default infroamation date expression for weekly jobs (Monday of the current week)
  default.weekly.output.info.date.expr = "lastMonday(@runDate)"

  # Default infroamation date expression for monthly jobs (The first day of the month)
  default.monthly.output.info.date.expr = "beginOfMonth(@runDate)"

  # Pramen will stop the Spark session at the end of execution by default. This helps to cleanly finalize running
  # jobs started from 'spark-submit'. But when running on Databriks this results in the job failure.
  stop.spark.session = true

  # Pramen will return a non-zero exit code on failures by default. But on Databricks this causes to end the job
  # prematurely and fail.
  exit.code.enabled = true

  timezone = "Africa/Johannesburg"
}

pramen.py {
  // Path to Pramen-Py (must be populates in order to suppot Pramen-Py)
  #location = ""
  executable = "pramen-py"

  cmd.line.template = "@location/@executable transformations run @pythonClass -c @metastoreConfig --info-date @infoDate"

  keep.log.lines = 2000
}

mail {
  # Any options from https://javaee.github.io/javamail/docs/api/com/sun/mail/smtp/package-summary.html

  #smtp.host = ""
  smtp.port = "25"
  smtp.auth = "false"
  smtp.starttls.enable = "false"
  smtp.EnableSSL.enable = "false"
  debug = "false"

  send.from = "Pramen <noreply@absa.africa>"
  send.to = ""
}

hadoop.redacted.tokens = [ password, secret, session.token ]

# Hadoop options to access S3
# hadoop.option {
#   # Authentication provider. Can be
#   # * org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider
#   # * com.amazonaws.auth.profile.ProfileCredentialsProvider
#   # * com.amazonaws.auth.InstanceProfileCredentialsProvider
#   # Use he default provider chain. It will use the first authentication provider that succeeds
#   fs.s3a.aws.credentials.provider = "com.amazonaws.auth.DefaultAWSCredentialsProviderChain"
#
#   # When an AWS profile is used from ~/.aws, you can set
#   # AWS_PROFILE to specify the exact profile and
#   # AWS_CREDENTIAL_PROFILES_FILE (and AWS_CONFIG_FILE) to specify the .aws config file if it is not ~/.aws/credentials

#   # Enable bucket key encruption
#   fs.s3a.server-side-encryption-bucket-key-enabled = "true"
#
#   # Enable magic committer for all buckets to have the best tradeoff between performance and safety
#   fs.s3a.committer.name = "magic"
#   fs.s3a.committer.magic.enabled = "true"
#
#   # Explicitly specify the endpoint
#   # fs.s3a.endpoint = "s3.af-south-1.amazonaws.com"
#
#   # AWS credentials
#   # fs.s3a.access.key = "AAABBBAAABBBAAABBBAA111")
#   # fs.s3a.secret.key = "abc123abc123abc123abc123abc123abc123"
#   # The session token for temporary credentials spec
#   # fs.s3a.session.token = ""
# }

# Java X configuration (for accessing services vis HTTPS)

# javax.net.ssl.trustStore = ""
# javax.net.ssl.trustStorePassword = ""
# javax.net.ssl.keyStore = ""
# javax.net.ssl.keyStorePassword = ""
# javax.net.ssl.password = ""
# java.security.auth.login.config = ""
# java.security.krb5.conf = ""
# javax.net.debug = ""

