# Copyright 2022 ABSA Group Limited
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# In order to run this example you should heva 'spark-cobol' in your classpath:
#   https://github.com/AbsaOSS/cobrix
# The embedded copybook is
#   https://github.com/AbsaOSS/cobrix/blob/master/data/test1_copybook.cob
# and the example data file is:
#   https://github.com/AbsaOSS/cobrix/tree/master/data/test1_data

pramen {
  run.type = "(Prod)"
  environment.name = "MyEnv "${pramen.run.type}
  pipeline.name = "My DCE pipeline"

  parallel.tasks = 1

  bookkeeping.enabled = false

  email.if.no.changes = false

  temporary.directory = "/tmp/pramen"
}

mail {
  ## FILL IN THE SMTP SERVER HOST NAME IN ORDER O RECEIVE NOTIFICATION EMAILS
  smtp.host = ""
  smtp.port = "25"
  smtp.auth = "false"
  smtp.starttls.enable = "false"
  smtp.EnableSSL.enable = "false"
  debug = "false"

  send.from = "Pramen <pramen.noreply@absa.africa>"

  ## FILL IN EMAIL RECEPIENTS
  send.to = ""
}

pramen.metastore {
  tables = [
     # We don't need to define metastore tables if we are using only transfer jobs.
  ]
}

pramen.sources = [
  {
    name = "mainframe_cobol"
    factory.class = "za.co.absa.pramen.core.source.SparkSource"

    format = "cobol"
    has.information.date.column = false

    option {
      copybook_contents = """
      01  RECORD.
          05  ID                        PIC S9(4)  COMP.
          05  COMPANY.
              10  SHORT-NAME            PIC X(10).
              10  COMPANY-ID-NUM        PIC 9(5) COMP-3.
              10  COMPANY-ID-STR
                  REDEFINES  COMPANY-ID-NUM PIC X(3).
          05  METADATA.
              10  CLIENTID              PIC X(15).
              10  REGISTRATION-NUM      PIC X(10).
              10  NUMBER-OF-ACCTS       PIC 9(03) COMP-3.
              10  ACCOUNT.
                  12  ACCOUNT-DETAIL    OCCURS 80
                                        DEPENDING ON NUMBER-OF-ACCTS.
                     15  ACCOUNT-NUMBER     PIC X(24).
                     15  ACCOUNT-TYPE-N     PIC 9(5) COMP-3.
                     15  ACCOUNT-TYPE-X     REDEFINES
                          ACCOUNT-TYPE-N  PIC X(3).

       """
    }
  },
]

pramen.sinks = [
  {
    name = "dce"
    factory.class = "za.co.absa.pramen.extras.sink.EnceladusSink"

    format = "csv"

    option {
      sep = "|"
      quoteAll = "false"
      header = "false"
    }

    mode = "overwrite"

    partition.pattern = "{year}/{month}/{day}/v{version}"

    records.per.partition = 1000000

    # Optional S3 version buckets cleanup via a special REST API
    cleanup.api.url = "https://hostname/api/path"
    cleanup.api.key = "aabbccdd"
    cleanup.api.trust.all.ssl.certificates = false

    info.file {
      generate = true

      source.application = "MyApp"
      country = "Africa"
      history.type = "Snapshot"
      timestamp.format = "dd-MM-yyyy HH:mm:ss Z"
      date.format = "yyyy-MM-dd"
    }
  }
]

pramen.operations = [
  {
    name = "My daily mainframe sourcing"
    type = "transfer"
    #disabled = "true"

    schedule.type = "daily"

    # Get data from the source and write it to the sink
    source = "mainframe_cobol"
    sink = "dce"

    # We are getting yesterday's data each day
    info.date.expr = "@runDate - 1"

    tables = [
      {
        # This needs to be set of the input and output are both paths
        job.metastore.table = "my_table1"

        input.path = "s3://mybucket/mainframe/landing/my_table1"
        output.path = "s3://mybucket/mainframe/raw/my_table1"

        output.info.version = 1

        # Optionally you can specify expressions for date ranges.
        date.from = "@infoDate"
        date.to = "@infoDate"
      }
    ]
  }
]
