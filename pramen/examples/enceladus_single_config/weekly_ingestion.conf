# Copyright 2022 ABSA Group Limited
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

pramen {
  run.type = "(Prod)"
  environment.name = "MyEnv "${pramen.run.type}
  pipeline.name = "My DCE pipeline"

  parallel.tasks = 1

  bookkeeping.enabled = false

  email.if.no.changes = false

  temporary.directory = "/tmp/pramen"
}

mail {
  ## FILL IN THE SMTP SERVER HOST NAME IN ORDER O RECEIVE NOTIFICATION EMAILS
  smtp.host = ""
  smtp.port = "25"
  smtp.auth = "false"
  smtp.starttls.enable = "false"
  smtp.EnableSSL.enable = "false"
  debug = "false"

  send.from = "Pramen <pramen.noreply@absa.africa>"

  ## FILL IN EMAIL RECEPIENTS
  send.to = ""
}

pramen.metastore {
  tables = [
     # We don't need to define metastore tables if we are using only transfer jobs.
  ]
}

pramen.sources = [
  {
    name = "postgre"
    factory.class = "za.co.absa.pramen.core.source.JdbcSource"

    jdbc = {
      driver = "org.postgresql.Driver"
      connection.primary.url = "jdbc:postgresql://host/test_db"
      user = ""
      password = ""
    }

    option.fetchsize = 50000
    option.batchsize = 50000

    # Specifies the minimum records the data should have to be considered having data
    minimum.records = 1

    # Consider the pipeline as failed if at least one table has no data at the scheduled time.
    # Useful for auto-retrying ingestion pipelines.
    fail.if.no.data = false

    has.information.date.column = true

    information.date.column = "info_date"
    information.date.type = "date"
    information.date.format = "yyyy-MM-dd"
  }
]

pramen.sinks = [
  {
    name = "dce"
    factory.class = "za.co.absa.pramen.extras.sink.EnceladusSink"

    format = "csv"

    option {
      sep = "|"
      quoteAll = "false"
      header = "false"
    }

    mode = "overwrite"

    partition.pattern = "{year}/{month}/{day}/v{version}"

    records.per.partition = 1000000

    # Optional S3 version buckets cleanup via a special REST API
    cleanup.api.url = "https://hostname/api/path"
    cleanup.api.key = "aabbccdd"
    cleanup.api.trust.all.ssl.certificates = false

    info.file {
      generate = true

      source.application = "MyApp"
      country = "Africa"
      history.type = "Snapshot"
      timestamp.format = "dd-MM-yyyy HH:mm:ss Z"
      date.format = "yyyy-MM-dd"
    }
  }
]

pramen.operations = [
  {
    name = "My weekly sourcing"
    type = "transfer"
    #disabled = "true"

    schedule.type = "weekly"
    schedule.days.of.week = [ 7 ] # On Sundays

    # Get data from the source and write it to the sink
    source = "postgre"
    sink = "dce"

    # Information date the output data is partitioned by is going to be saturday
    # based on the day the jobs should normally run (Sunday)
    info.date.expr = "lastSaturday(@runDate)"

    tables = [
      {
        input.db.table = my_table1
        output.path = "/bigdata/datalake/raw/my_table1"

        # Autodetect info version based on files in the raw and publish folders
        # Needs 'output.publish.base.path' or 'output.hive.table' to be set
        output.info.version = auto

        output.dataset.name = "my_dataset"

        output {
           # Optional when running Enceladus from Pramen
           dataset.name = "my_dataset"
           dataset.version = 2

           # Optional publish base path (for detecting version number)
           publish.base.path = "/bigdata/datalake/publish"
           # Optional Hive table to repair after Enceladus is executed
           hive.table = "my_database.my_table"
        }

        # Optionally you can specify expressions for date ranges.
        date.from = "@infoDate"
        date.to = "@infoDate"
      },
      {
        input.db.table = my_table2
        output.path = "/bigdata/datalake/raw/my_table2"
        output.info.version = 1

        # Optionally you can specify expressions for date ranges.
        date.from = "@infoDate"
        date.to = "@infoDate"
      }
    ]
  }
]
