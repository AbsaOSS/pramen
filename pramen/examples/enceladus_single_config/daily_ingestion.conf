# Copyright 2022 ABSA Group Limited
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

pramen {
  environment.name = "(Prod) MyEnvName"
  pipeline.name = "My DCE pipeline"

  bookkeeping.enabled = false

  email.if.no.changes = false

  temporary.directory = "/tmp/pramen"

  parallel.tasks = 1
}

mail {
  ## FILL IN THE SMTP SERVER HOST NAME IN ORDER O RECEIVE NOTIFICATION EMAILS
  smtp.host = ""
  smtp.port = "25"
  smtp.auth = "false"
  smtp.starttls.enable = "false"
  smtp.EnableSSL.enable = "false"
  debug = "false"

  send.from = "Pramen <pramen.noreply@absa.africa>"

  ## FILL IN EMAIL RECEPIENTS
  send.to = ""
}

pramen.metastore {
  tables = [
     # We don't need to define metastore tables if we are using only transfer jobs.
  ]
}

pramen.sources = [
  {
    name = "my_source"
    factory.class = "za.co.absa.pramen.core.source.JdbcSource"

    jdbc = {
      driver = "org.postgresql.Driver"
      connection.primary.url = "jdbc:postgresql://host/test_db"
      user = ""
      password = ""
    }

    option.fetchsize = 50000
    option.batchsize = 50000

    # Specifies the minimum records the data should have to be considered having data
    minimum.records = 1

    # Consider the pipeline as failed if at least one table has no data at the scheduled time.
    # Useful for auto-retrying ingestion pipelines.
    fail.if.no.data = false

    has.information.date.column = true

    information.date.column = "info_date"
    information.date.type = "date"
    information.date.format = "yyyy-MM-dd"
  }
]

pramen.sinks = [
  {
    name = "dce"
    factory.class = "za.co.absa.pramen.extras.sink.EnceladusSink"

    format = "parquet"

    mode = "overwrite"

    records.per.partition = 1000000

    # Information date column, default: enceladus_info_date
    info.date.column = "enceladus_info_date"

    # Partition pattern. Default: {year}/{month}/{day}/v{version}
    partition.pattern = "{year}/{month}/{day}/v{version}"

    # If true (default), the data will be saved even if it does not contain any records. If false, the saving will be skipped
    save.empty = true

    # Setup Enceladus main class and command line template if you want to run it from Pramen
    enceladus.run.main.class = "za.co.absa.enceladus.standardization_conformance.StandardizationAndConformanceJob"

    # Command line template for Enceladus
    # You can use the following variables: @datasetName, @datasetName, @datasetVersion, @infoDate, @infoVersion, @rawPath, @rawFormat.
    enceladus.command.line.template = "--autoclean-std-folder true --dataset-name @datasetName --dataset-version @datasetVersion --report-date @infoDate --report-version @infoVersion --menas-auth-keytab menas.keytab --raw-format @rawFormat"

    hive.conf {
      # You can specify Hive table creation template if it is different from the default one.
      create.table.template = """CREATE EXTERNAL TABLE IF NOT EXISTS
@fullTableName ( @schema )
ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'
@partitionedBy
LOCATION '@path'"""
    }

    # Optional S3 version buckets cleanup via a special REST API
    cleanup.api.url = "https://hostname/api/path"
    cleanup.api.key = "aabbccdd"
    cleanup.api.trust.all.ssl.certificates = false

    info.file {
      generate = true

      source.application = "MyApp"
      country = "Africa"
      history.type = "Snapshot"
      timestamp.format = "dd-MM-yyyy HH:mm:ss Z"
      date.format = "yyyy-MM-dd"
    }
  }
]

pramen.operations = [
  {
    name = "My daily sourcing"
    type = "transfer"
    #disabled = "true"

    # Get data from the source and write it to the sink
    source = "my_source"
    sink = "dce"

    schedule.type = "daily"

    # The data will be loaded for the same date as it is ran
    info.date.expr = "@runDate"

    tables = [
      {
        # The table in the database
        input.db.table = my_table1
        # The path to the raw folder in HDFS
        output.path = "/mydatalake/raw/my_table1"
        # The path to the publish folder in HDFS
        output.publish.base.path = "/mydatalake/publish/my_table1"

        # These parameters should point to the info in Menas
        output.dataset.name="my_dataset"
        output.dataset.version = 1
        output.hive.table = "my_hive_database.my_hive_table"
        output.info.version = 1

        # Optionally you can specify expressions for date ranges.
        date.from = "@infoDate"
        date.to = "@infoDate"
      }
    ]
  }
]
